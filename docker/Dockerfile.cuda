# CUDA variant - optimized for NVIDIA GPUs (RTX/GTX/Ada Lovelace)
# Requires NVIDIA Container Toolkit
FROM nvidia/cuda:12.4.1-runtime-ubuntu22.04

WORKDIR /backend

# 1. System Dependencies - use what base image provides
# Try apt first, but don't fail if it doesn't work
RUN apt-get update --allow-insecure-repositories 2>/dev/null || true \
    && apt-get install -y --allow-unauthenticated \
    python3 python3-pip python3-venv \
    libgomp1 libstdc++6 curl git 2>/dev/null || true \
    && rm -rf /var/lib/apt/lists/* || true

# Check for Python, use what's available
RUN command -v python3 >/dev/null 2>&1 || command -v python >/dev/null 2>&1 || { echo "Python not found"; exit 1; }
RUN python3 --version >/dev/null 2>&1 || python --version >/dev/null 2>&1

# Try to get pip
RUN python3 -m ensurepip --upgrade 2>/dev/null || python -m ensurepip --upgrade 2>/dev/null || true
RUN command -v pip3 >/dev/null 2>&1 || command -v pip >/dev/null 2>&1 || python3 -m pip --version >/dev/null 2>&1 || python -m pip --version >/dev/null 2>&1 || true

# 2. Python Environment
ENV PYTHONUNBUFFERED=1 PIP_ROOT_USER_ACTION=ignore

RUN python3 -m pip install --no-cache-dir --upgrade pip setuptools wheel || python -m pip install --no-cache-dir --upgrade pip setuptools wheel || true

# 3. Install PyTorch with CUDA 12.1
RUN python3 -m pip install --no-cache-dir \
    torch torchvision --index-url https://download.pytorch.org/whl/cu121

# 4. Install llama-cpp-python with CUDA support
RUN python3 -m pip install --no-cache-dir llama-cpp-python

# 5. Install Python dependencies
COPY backend/requirements.txt /tmp/requirements.txt
RUN python3 -m pip install --no-cache-dir -r /tmp/requirements.txt

# 6. Copy Application Code
COPY backend/ ./backend/

# 7. Create Persistent Directories
RUN mkdir -p /backend/data/qdrant_db \
             /backend/media/uploads \
             /backend/data/memory \
             /backend/data/models \
             /backend/app/persistent_build

# 8. Environment
ENV PYTHONPATH=/backend
ENV CONFIG_PATH=/backend/config.yaml
ENV FORCED_BACKEND=cuda

EXPOSE 8000

COPY docker-entrypoint.sh /
RUN chmod +x /docker-entrypoint.sh
ENTRYPOINT ["/docker-entrypoint.sh"]
